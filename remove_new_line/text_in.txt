Synthetic Open Data. One solution to the above is to exploit synthetic data, armed with which we might expect the classifier D to perform better. Assume we have a generator network G(z) that produces synthetic images given (Gaussian normal) random noise inputs z âˆ¼ N . We can naively add them to the pool of negative or open-set examples that D should not fire on. But these synthetic images might be too easy for D to categorize as open-set data [32, 10]. A natural solution is to adversarially train the generator G to produce difficult examples that fool the classifier D using a GAN loss